{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.设置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"ptb.train\"          # 训练数据路径。\n",
    "EVAL_DATA = \"ptb.valid\"           # 验证数据路径。\n",
    "TEST_DATA = \"ptb.test\"            # 测试数据路径。\n",
    "HIDDEN_SIZE = 300                 # 隐藏层规模。\n",
    "NUM_LAYERS = 2                    # 深层循环神经网络中LSTM结构的层数。\n",
    "VOCAB_SIZE = 10000                # 词典规模。\n",
    "TRAIN_BATCH_SIZE = 20             # 训练数据batch的大小。\n",
    "TRAIN_NUM_STEP = 35               # 训练数据截断长度。\n",
    "\n",
    "EVAL_BATCH_SIZE = 1               # 测试数据batch的大小。\n",
    "EVAL_NUM_STEP = 1                 # 测试数据截断长度。\n",
    "NUM_EPOCH = 5                     # 使用训练数据的轮数。\n",
    "LSTM_KEEP_PROB = 0.9              # LSTM节点不被dropout的概率。\n",
    "EMBEDDING_KEEP_PROB = 0.9         # 词向量不被dropout的概率。\n",
    "MAX_GRAD_NORM = 5                 # 用于控制梯度膨胀的梯度大小上限。\n",
    "SHARE_EMB_AND_SOFTMAX = True      # 在Softmax层和词向量层之间共享参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.定义模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过一个PTBModel类来描述模型，这样方便维护循环神经网络中的状态。\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        # 记录使用的batch大小和截断长度。\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        # 定义每一步的输入和预期输出。两者的维度都是[batch_size, num_steps]。\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 定义使用LSTM结构为循环体结构且使用dropout的深层循环神经网络。\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "            for _ in range(NUM_LAYERS)]     \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)            \n",
    "        \n",
    "        # 初始化最初的状态，即全零的向量。这个量只在每个epoch初始化第一个batch\n",
    "        # 时使用。\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # 定义单词的词向量矩阵。\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "        # 将输入单词转化为词向量。\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        \n",
    "        # 只在训练时使用dropout。\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)\n",
    " \n",
    "        # 定义输出列表。在这里先将不同时刻LSTM结构的输出收集起来，再一起提供给\n",
    "        # softmax层。\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output) \n",
    "        # 把输出队列展开成[batch, hidden_size*num_steps]的形状，然后再\n",
    "        # reshape成[batch*numsteps, hidden_size]的形状。\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    " \n",
    "        # Softmax层：将RNN在每个位置上的输出转化为各个单词的logits。\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            weight = tf.transpose(embedding)\n",
    "        else:\n",
    "            weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        # 定义交叉熵损失函数和平均损失。\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(self.targets, [-1]),\n",
    "            logits=logits)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 只在训练模型时定义反向传播操作。\n",
    "        if not is_training: return\n",
    "\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        # 控制梯度大小，定义优化方法和训练步骤。\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.定义数据和训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用给定的模型model在数据data上运行train_op并返回在全部数据上的perplexity值。\n",
    "def run_epoch(session, model, batches, train_op, output_log, step):\n",
    "    # 计算平均perplexity的辅助变量。\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state) \n",
    "    # 训练一个epoch。\n",
    "    for x, y in batches:\n",
    "        # 在当前batch上运行train_op并计算损失值。交叉熵损失函数计算的就是下一个单\n",
    "        # 词为给定单词的概率。\n",
    "        cost, state, _ = session.run(\n",
    "             [model.cost, model.final_state, train_op],\n",
    "             {model.input_data: x, model.targets: y,\n",
    "              model.initial_state: state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        # 只有在训练时输出日志。\n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After %d steps, perplexity is %.3f\" % (\n",
    "                  step, np.exp(total_costs / iters)))\n",
    "        step += 1\n",
    "\n",
    "    # 返回给定模型在给定数据上的perplexity值。\n",
    "    return step, np.exp(total_costs / iters)\n",
    "\n",
    "\n",
    "# 从文件中读取数据，并返回包含单词编号的数组。\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        # 将整个文档读进一个长字符串。\n",
    "        id_string = ' '.join([line.strip() for line in fin.readlines()])\n",
    "    id_list = [int(w) for w in id_string.split()]  # 将读取的单词编号转为整数\n",
    "    return id_list\n",
    "\n",
    "\n",
    "def make_batches(id_list, batch_size, num_step):\n",
    "    # 计算总的batch数量。每个batch包含的单词数量是batch_size * num_step。\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step)\n",
    "\n",
    "    # 如9-4图所示，将数据整理成一个维度为[batch_size, num_batches * num_step]\n",
    "    # 的二维数组。\n",
    "    data = np.array(id_list[: num_batches * batch_size * num_step])\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step])\n",
    "    # 沿着第二个维度将数据切分成num_batches个batch，存入一个数组。\n",
    "    data_batches = np.split(data, num_batches, axis=1)\n",
    "\n",
    "    # 重复上述操作，但是每个位置向右移动一位。这里得到的是RNN每一步输出所需要预测的\n",
    "    # 下一个单词。\n",
    "    label = np.array(id_list[1 : num_batches * batch_size * num_step + 1]) \n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)  \n",
    "    # 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵。\n",
    "    return list(zip(data_batches, label_batches)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.主函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0620 10:50:53.285612 140034454939392 deprecation.py:323] From <ipython-input-3-5a4a8ef4c463>:18: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0620 10:50:53.291328 140034454939392 deprecation.py:323] From <ipython-input-3-5a4a8ef4c463>:19: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0620 10:50:53.319817 140034454939392 deprecation.py:506] From <ipython-input-3-5a4a8ef4c463>:33: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0620 10:50:53.533518 140034454939392 deprecation.py:506] From /home/yxq/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0620 10:50:57.488909 140034454939392 deprecation.py:323] From /home/yxq/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, perplexity is 10015.837\n",
      "After 100 steps, perplexity is 1695.231\n",
      "After 200 steps, perplexity is 1146.954\n",
      "After 300 steps, perplexity is 901.769\n",
      "After 400 steps, perplexity is 740.132\n",
      "After 500 steps, perplexity is 630.950\n",
      "After 600 steps, perplexity is 558.570\n",
      "After 700 steps, perplexity is 502.796\n",
      "After 800 steps, perplexity is 454.284\n",
      "After 900 steps, perplexity is 418.323\n",
      "After 1000 steps, perplexity is 392.400\n",
      "After 1100 steps, perplexity is 366.375\n",
      "After 1200 steps, perplexity is 345.977\n",
      "After 1300 steps, perplexity is 326.532\n",
      "Epoch: 1 Train Perplexity: 323.530\n",
      "Epoch: 1 Eval Perplexity: 183.305\n",
      "In iteration: 2\n",
      "After 1400 steps, perplexity is 177.713\n",
      "After 1500 steps, perplexity is 164.472\n",
      "After 1600 steps, perplexity is 166.747\n",
      "After 1700 steps, perplexity is 163.725\n",
      "After 1800 steps, perplexity is 159.150\n",
      "After 1900 steps, perplexity is 157.085\n",
      "After 2000 steps, perplexity is 155.354\n",
      "After 2100 steps, perplexity is 150.554\n",
      "After 2200 steps, perplexity is 147.613\n",
      "After 2300 steps, perplexity is 146.340\n",
      "After 2400 steps, perplexity is 144.011\n",
      "After 2500 steps, perplexity is 141.129\n",
      "After 2600 steps, perplexity is 137.710\n",
      "Epoch: 2 Train Perplexity: 137.147\n",
      "Epoch: 2 Eval Perplexity: 134.155\n",
      "In iteration: 3\n",
      "After 2700 steps, perplexity is 121.157\n",
      "After 2800 steps, perplexity is 106.421\n",
      "After 2900 steps, perplexity is 112.973\n",
      "After 3000 steps, perplexity is 110.702\n",
      "After 3100 steps, perplexity is 109.748\n",
      "After 3200 steps, perplexity is 109.894\n",
      "After 3300 steps, perplexity is 109.270\n",
      "After 3400 steps, perplexity is 107.308\n",
      "After 3500 steps, perplexity is 105.457\n",
      "After 3600 steps, perplexity is 105.048\n",
      "After 3700 steps, perplexity is 104.933\n",
      "After 3800 steps, perplexity is 102.932\n",
      "After 3900 steps, perplexity is 101.040\n",
      "Epoch: 3 Train Perplexity: 100.699\n",
      "Epoch: 3 Eval Perplexity: 115.976\n",
      "In iteration: 4\n",
      "After 4000 steps, perplexity is 99.072\n",
      "After 4100 steps, perplexity is 84.386\n",
      "After 4200 steps, perplexity is 89.631\n",
      "After 4300 steps, perplexity is 89.613\n",
      "After 4400 steps, perplexity is 88.790\n",
      "After 4500 steps, perplexity is 88.407\n",
      "After 4600 steps, perplexity is 88.121\n",
      "After 4700 steps, perplexity is 87.388\n",
      "After 4800 steps, perplexity is 86.076\n",
      "After 4900 steps, perplexity is 85.640\n",
      "After 5000 steps, perplexity is 85.943\n",
      "After 5100 steps, perplexity is 84.559\n",
      "After 5200 steps, perplexity is 83.599\n",
      "After 5300 steps, perplexity is 83.136\n",
      "Epoch: 4 Train Perplexity: 83.110\n",
      "Epoch: 4 Eval Perplexity: 109.619\n",
      "In iteration: 5\n",
      "After 5400 steps, perplexity is 73.608\n",
      "After 5500 steps, perplexity is 75.145\n",
      "After 5600 steps, perplexity is 78.516\n",
      "After 5700 steps, perplexity is 76.498\n",
      "After 5800 steps, perplexity is 75.454\n",
      "After 5900 steps, perplexity is 75.548\n",
      "After 6000 steps, perplexity is 75.668\n",
      "After 6100 steps, perplexity is 74.420\n",
      "After 6200 steps, perplexity is 74.119\n",
      "After 6300 steps, perplexity is 74.646\n",
      "After 6400 steps, perplexity is 73.985\n",
      "After 6500 steps, perplexity is 73.312\n",
      "After 6600 steps, perplexity is 72.432\n",
      "Epoch: 5 Train Perplexity: 72.624\n",
      "Epoch: 5 Eval Perplexity: 108.686\n",
      "Test Perplexity: 104.536\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 定义初始化函数。\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    \n",
    "    # 定义训练用的循环神经网络模型。\n",
    "    with tf.variable_scope(\"language_model\", \n",
    "                           reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "\n",
    "    # 定义测试用的循环神经网络模型。它与train_model共用参数，但是没有dropout。\n",
    "    with tf.variable_scope(\"language_model\",\n",
    "                           reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "    # 训练模型。\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        train_batches = make_batches(\n",
    "            read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "        eval_batches = make_batches(\n",
    "            read_data(EVAL_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "        test_batches = make_batches(\n",
    "            read_data(TEST_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "        step = 0\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: %d\" % (i + 1))\n",
    "            step, train_pplx = run_epoch(session, train_model, train_batches,\n",
    "                                         train_model.train_op, True, step)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_pplx))\n",
    "\n",
    "            _, eval_pplx = run_epoch(session, eval_model, eval_batches,\n",
    "                                     tf.no_op(), False, 0)\n",
    "            print(\"Epoch: %d Eval Perplexity: %.3f\" % (i + 1, eval_pplx))\n",
    "\n",
    "        _, test_pplx = run_epoch(session, eval_model, test_batches,\n",
    "                                 tf.no_op(), False, 0)\n",
    "        print(\"Test Perplexity: %.3f\" % test_pplx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
